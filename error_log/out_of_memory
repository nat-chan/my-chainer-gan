Batch size: 100######.........................................] 19.99%
Total number of images: 50000############.....................] 58.72%
Total number of batches: 500100000 iterations
Running batch 1 / 500 ...imated time to finish: 2:51:53.017005.
Exception in main training loop: out of memory to allocate 553190400 bytes (total 3161046528 bytes)
Traceback (most recent call last):
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/training/trainer.py", line 302, in run
    entry.extension(self)
  File "/home/n/dev/chainer-gan-lib/common/evaluation.py", line 102, in evaluation
    mean, std = inception_score(model, ims)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 50, in inception_score
    y = model(ims_batch)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 540, in __call__
    h = F.relu(self.bn_conv_2(self.conv_2(h)))
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/links/normalization/batch_normalization.py", line 149, in __call__
    x, gamma, beta, mean, var, self.eps)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 610, in fixed_batch_normalization
    return FixedBatchNormalization(eps).apply((x, gamma, beta, mean, var))[0]
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/function_node.py", line 225, in apply
    outputs = self.forward(in_data)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 324, in forward
    y = cuda.cupy.empty_like(x)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/cupy/creation/basic.py", line 41, in empty_like
    return cupy.ndarray(a.shape, dtype=dtype)
  File "cupy/core/core.pyx", line 94, in cupy.core.core.ndarray.__init__
  File "cupy/cuda/memory.pyx", line 392, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 800, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 821, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 622, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 672, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 654, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 592, in cupy.cuda.memory.SingleDeviceMemoryPool._alloc
  File "cupy/cuda/memory.pyx", line 347, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 348, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 45, in cupy.cuda.memory.Memory.__init__
  File "cupy/cuda/runtime.pyx", line 214, in cupy.cuda.runtime.malloc
  File "cupy/cuda/runtime.pyx", line 137, in cupy.cuda.runtime.check_status
cupy.cuda.runtime.CUDARuntimeError: cudaErrorMemoryAllocation: out of memory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 660, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 592, in cupy.cuda.memory.SingleDeviceMemoryPool._alloc
  File "cupy/cuda/memory.pyx", line 347, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 348, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 45, in cupy.cuda.memory.Memory.__init__
  File "cupy/cuda/runtime.pyx", line 214, in cupy.cuda.runtime.malloc
  File "cupy/cuda/runtime.pyx", line 137, in cupy.cuda.runtime.check_status
cupy.cuda.runtime.CUDARuntimeError: cudaErrorMemoryAllocation: out of memory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 666, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 592, in cupy.cuda.memory.SingleDeviceMemoryPool._alloc
  File "cupy/cuda/memory.pyx", line 347, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 348, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 45, in cupy.cuda.memory.Memory.__init__
  File "cupy/cuda/runtime.pyx", line 214, in cupy.cuda.runtime.malloc
  File "cupy/cuda/runtime.pyx", line 137, in cupy.cuda.runtime.check_status
cupy.cuda.runtime.CUDARuntimeError: cudaErrorMemoryAllocation: out of memory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 191, in <module>
    main()
  File "train.py", line 187, in main
    trainer.run()
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/training/trainer.py", line 313, in run
    six.reraise(*sys.exc_info())
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/six.py", line 693, in reraise
    raise value
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/training/trainer.py", line 302, in run
    entry.extension(self)
  File "/home/n/dev/chainer-gan-lib/common/evaluation.py", line 102, in evaluation
    mean, std = inception_score(model, ims)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 50, in inception_score
    y = model(ims_batch)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 540, in __call__
    h = F.relu(self.bn_conv_2(self.conv_2(h)))
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/links/normalization/batch_normalization.py", line 149, in __call__
    x, gamma, beta, mean, var, self.eps)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 610, in fixed_batch_normalization
    return FixedBatchNormalization(eps).apply((x, gamma, beta, mean, var))[0]
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/function_node.py", line 225, in apply
    outputs = self.forward(in_data)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 324, in forward
    y = cuda.cupy.empty_like(x)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/cupy/creation/basic.py", line 41, in empty_like
    return cupy.ndarray(a.shape, dtype=dtype)
  File "cupy/core/core.pyx", line 94, in cupy.core.core.ndarray.__init__
  File "cupy/cuda/memory.pyx", line 392, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 800, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 821, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 622, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 672, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
cupy.cuda.memory.OutOfMemoryError: out of memory to allocate 553190400 bytes (total 3161046528 bytes)


#BEGAN

Batch size: 100#..............................................]  9.99%
Total number of images: 50000######################...........] 78.72%
Total number of batches: 500100000 iterations
Running batch 1 / 500 ...imated time to finish: 3:31:08.048218.
Running batch 2 / 500 ...
Exception in main training loop: out of memory to allocate 553190400 bytes (total 2741408768 bytes)
Traceback (most recent call last):
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/training/trainer.py", line 302, in run
    entry.extension(self)
  File "/home/n/dev/chainer-gan-lib/common/evaluation.py", line 102, in evaluation
    mean, std = inception_score(model, ims)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 50, in inception_score
    y = model(ims_batch)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 540, in __call__
    h = F.relu(self.bn_conv_2(self.conv_2(h)))
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/links/normalization/batch_normalization.py", line 149, in __call__
    x, gamma, beta, mean, var, self.eps)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 610, in fixed_batch_normalization
    return FixedBatchNormalization(eps).apply((x, gamma, beta, mean, var))[0]
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/function_node.py", line 225, in apply
    outputs = self.forward(in_data)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 324, in forward
    y = cuda.cupy.empty_like(x)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/cupy/creation/basic.py", line 41, in empty_like
    return cupy.ndarray(a.shape, dtype=dtype)
  File "cupy/core/core.pyx", line 94, in cupy.core.core.ndarray.__init__
  File "cupy/cuda/memory.pyx", line 392, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 800, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 821, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 622, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 672, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
Will finalize trainer extensions and updater before reraising the exception.
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 654, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 592, in cupy.cuda.memory.SingleDeviceMemoryPool._alloc
  File "cupy/cuda/memory.pyx", line 347, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 348, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 45, in cupy.cuda.memory.Memory.__init__
  File "cupy/cuda/runtime.pyx", line 214, in cupy.cuda.runtime.malloc
  File "cupy/cuda/runtime.pyx", line 137, in cupy.cuda.runtime.check_status
cupy.cuda.runtime.CUDARuntimeError: cudaErrorMemoryAllocation: out of memory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 660, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 592, in cupy.cuda.memory.SingleDeviceMemoryPool._alloc
  File "cupy/cuda/memory.pyx", line 347, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 348, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 45, in cupy.cuda.memory.Memory.__init__
  File "cupy/cuda/runtime.pyx", line 214, in cupy.cuda.runtime.malloc
  File "cupy/cuda/runtime.pyx", line 137, in cupy.cuda.runtime.check_status
cupy.cuda.runtime.CUDARuntimeError: cudaErrorMemoryAllocation: out of memory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 666, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
  File "cupy/cuda/memory.pyx", line 592, in cupy.cuda.memory.SingleDeviceMemoryPool._alloc
  File "cupy/cuda/memory.pyx", line 347, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 348, in cupy.cuda.memory._malloc
  File "cupy/cuda/memory.pyx", line 45, in cupy.cuda.memory.Memory.__init__
  File "cupy/cuda/runtime.pyx", line 214, in cupy.cuda.runtime.malloc
  File "cupy/cuda/runtime.pyx", line 137, in cupy.cuda.runtime.check_status
cupy.cuda.runtime.CUDARuntimeError: cudaErrorMemoryAllocation: out of memory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 191, in <module>
    main()
  File "train.py", line 187, in main
    trainer.run()
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/training/trainer.py", line 313, in run
    six.reraise(*sys.exc_info())
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/six.py", line 693, in reraise
    raise value
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/training/trainer.py", line 302, in run
    entry.extension(self)
  File "/home/n/dev/chainer-gan-lib/common/evaluation.py", line 102, in evaluation
    mean, std = inception_score(model, ims)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 50, in inception_score
    y = model(ims_batch)
  File "/home/n/dev/chainer-gan-lib/common/inception/inception_score.py", line 540, in __call__
    h = F.relu(self.bn_conv_2(self.conv_2(h)))
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/links/normalization/batch_normalization.py", line 149, in __call__
    x, gamma, beta, mean, var, self.eps)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 610, in fixed_batch_normalization
    return FixedBatchNormalization(eps).apply((x, gamma, beta, mean, var))[0]
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/function_node.py", line 225, in apply
    outputs = self.forward(in_data)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/chainer/functions/normalization/batch_normalization.py", line 324, in forward
    y = cuda.cupy.empty_like(x)
  File "/home/n/.pyenv/versions/3.5.2/lib/python3.5/site-packages/cupy/creation/basic.py", line 41, in empty_like
    return cupy.ndarray(a.shape, dtype=dtype)
  File "cupy/core/core.pyx", line 94, in cupy.core.core.ndarray.__init__
  File "cupy/cuda/memory.pyx", line 392, in cupy.cuda.memory.alloc
  File "cupy/cuda/memory.pyx", line 800, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 821, in cupy.cuda.memory.MemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 622, in cupy.cuda.memory.SingleDeviceMemoryPool.malloc
  File "cupy/cuda/memory.pyx", line 672, in cupy.cuda.memory.SingleDeviceMemoryPool._malloc
cupy.cuda.memory.OutOfMemoryError: out of memory to allocate 553190400 bytes (total 2741408768 bytes)
